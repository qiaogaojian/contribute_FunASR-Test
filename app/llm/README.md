# LLM Provideræ¨¡å—

ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹æ¥å£ï¼Œæ”¯æŒå¤šç§LLM Providerï¼Œæä¾›ä¸€è‡´çš„APIä½“éªŒã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”Œ **å¤šProvideræ”¯æŒ**: OpenAIã€Anthropicã€Googleã€Ollamaç­‰
- ğŸŒŠ **æµå¼å“åº”**: æ”¯æŒå®æ—¶æµå¼è¾“å‡º
- ğŸ› ï¸ **å·¥å…·è°ƒç”¨**: æ”¯æŒFunction Calling
- ğŸ“Š **æ¨¡å‹ä¿¡æ¯**: å®Œæ•´çš„æ¨¡å‹èƒ½åŠ›å’Œä»·æ ¼ä¿¡æ¯
- âš™ï¸ **é…ç½®ç®¡ç†**: çµæ´»çš„é…ç½®ç³»ç»Ÿ
- ğŸ”„ **å¼‚æ­¥æ”¯æŒ**: å®Œå…¨å¼‚æ­¥çš„APIè®¾è®¡
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## æ”¯æŒçš„Provider

| Provider | çŠ¶æ€ | æµå¼ | å·¥å…·è°ƒç”¨ | å›¾åƒæ”¯æŒ |
|----------|------|------|----------|----------|
| OpenAI | âœ… | âœ… | âœ… | âœ… |
| Anthropic | âœ… | âœ… | âœ… | âœ… |
| Google/Gemini | âœ… | âœ… | âŒ | âœ… |
| Ollama | âœ… | âœ… | âŒ | âŒ |
| Deepseek | âœ… | âœ… | âœ… | âŒ |
| OpenRouter | âœ… | âœ… | âœ… | âœ… |
| SiliconFlow | âœ… | âœ… | âœ… | âŒ |
| Groq | âœ… | âœ… | âœ… | âŒ |

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from app.llm import LLMManager, LLMRequest, LLMProviderConfig, ChatMessage, MessageRole, ApiProvider

async def basic_example():
    # åˆ›å»ºç®¡ç†å™¨
    manager = LLMManager()
    
    # é…ç½®Provider
    config = LLMProviderConfig(
        provider=ApiProvider.OPENAI,
        api_key="your-api-key"
    )
    
    # æ³¨å†ŒProvider
    manager.register_provider(ApiProvider.OPENAI, config)
    
    # åˆ›å»ºè¯·æ±‚
    request = LLMRequest(
        model="gpt-3.5-turbo",
        messages=[
            ChatMessage(role=MessageRole.USER, content="Hello, world!")
        ]
    )
    
    # ç”Ÿæˆå“åº”
    response = await manager.generate_response(request)
    print(response.content)

asyncio.run(basic_example())
```

### 2. æµå¼å“åº”

```python
async def streaming_example():
    # ... é…ç½®ä»£ç åŒä¸Š ...
    
    request = LLMRequest(
        model="gpt-3.5-turbo",
        messages=[ChatMessage(role=MessageRole.USER, content="å†™ä¸€é¦–è¯—")],
        stream=True
    )
    
    async for chunk in manager.stream_response(request):
        print(chunk.delta, end="", flush=True)
```

### 3. å¤šProviderä½¿ç”¨

```python
async def multi_provider_example():
    manager = LLMManager()
    
    # æ³¨å†Œå¤šä¸ªProvider
    providers = [
        (ApiProvider.OPENAI, {"api_key": "openai-key"}),
        (ApiProvider.ANTHROPIC, {"api_key": "anthropic-key"}),
        (ApiProvider.OLLAMA, {"base_url": "http://localhost:11434"})
    ]
    
    for provider_type, config_dict in providers:
        config = LLMProviderConfig(provider=provider_type, **config_dict)
        manager.register_provider(provider_type, config)
    
    # è‡ªåŠ¨é€‰æ‹©Provider
    request = LLMRequest(
        model="gpt-4o",  # ä¼šè‡ªåŠ¨é€‰æ‹©OpenAI
        messages=[ChatMessage(role=MessageRole.USER, content="Hello")]
    )
    
    response = await manager.generate_response(request)
```

## é…ç½®

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨`.env`æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```env
# é»˜è®¤LLMé…ç½®
ASR_LLM_PROVIDER=openai
ASR_LLM_MODEL=gpt-3.5-turbo
ASR_LLM_API_KEY=your-default-api-key
ASR_LLM_TEMPERATURE=0.7
ASR_LLM_MAX_TOKENS=1000

# å„Providerçš„APIå¯†é’¥
ASR_OPENAI_API_KEY=your-openai-key
ASR_ANTHROPIC_API_KEY=your-anthropic-key
ASR_GOOGLE_API_KEY=your-google-key
ASR_OLLAMA_BASE_URL=http://localhost:11434
```

### ä»£ç é…ç½®

```python
from app.llm import LLMProviderConfig, ApiProvider

config = LLMProviderConfig(
    provider=ApiProvider.OPENAI,
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",  # å¯é€‰
    timeout=30,
    max_retries=3,
    extra_config={"organization": "your-org"}  # Providerç‰¹å®šé…ç½®
)
```

## æ¨¡å‹ä¿¡æ¯

æŸ¥è¯¢æ¨¡å‹èƒ½åŠ›å’Œä»·æ ¼ä¿¡æ¯ï¼š

```python
# æŸ¥è¯¢ç‰¹å®šæ¨¡å‹
model_info = manager.get_model_info("gpt-4o")
print(f"æœ€å¤§tokens: {model_info.max_tokens}")
print(f"æ”¯æŒå›¾åƒ: {model_info.supports_images}")
print(f"è¾“å…¥ä»·æ ¼: ${model_info.input_price}/ç™¾ä¸‡tokens")

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
all_models = manager.list_available_models()
for name, info in all_models.items():
    print(f"{name}: {info.description}")
```

## é”™è¯¯å¤„ç†

```python
from app.llm.exceptions import (
    LLMError,
    LLMAuthenticationError,
    LLMRateLimitError,
    LLMModelNotFoundError
)

try:
    response = await manager.generate_response(request)
except LLMAuthenticationError:
    print("APIå¯†é’¥æ— æ•ˆ")
except LLMRateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶ï¼Œè¯·ç­‰å¾… {e.retry_after} ç§’")
except LLMModelNotFoundError:
    print("æ¨¡å‹ä¸å­˜åœ¨")
except LLMError as e:
    print(f"LLMé”™è¯¯: {e}")
```

## é«˜çº§åŠŸèƒ½

### å·¥å…·è°ƒç”¨

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "åŸå¸‚åç§°"}
                },
                "required": ["location"]
            }
        }
    }
]

request = LLMRequest(
    model="gpt-4o",
    messages=[ChatMessage(role=MessageRole.USER, content="åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ")],
    tools=tools,
    tool_choice="auto"
)
```

### ä¸Šä¸‹æ–‡ç®¡ç†

```python
async with manager.provider_context(ApiProvider.OPENAI) as provider:
    # ç›´æ¥ä½¿ç”¨Provider
    response = await provider.generate_response(request)
```

## æ‰©å±•æ–°Provider

1. ç»§æ‰¿`BaseLLMProvider`ç±»
2. å®ç°å¿…è¦çš„æŠ½è±¡æ–¹æ³•
3. åœ¨`LLMManager`ä¸­æ³¨å†Œ

```python
from app.llm.base import BaseLLMProvider

class CustomProvider(BaseLLMProvider):
    async def generate_response(self, request, **kwargs):
        # å®ç°ç”Ÿæˆé€»è¾‘
        pass
    
    async def stream_response(self, request, **kwargs):
        # å®ç°æµå¼é€»è¾‘
        pass
    
    def get_available_models(self):
        # è¿”å›å¯ç”¨æ¨¡å‹
        pass
    
    def validate_config(self):
        # éªŒè¯é…ç½®
        pass
```

## æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥å®‰å…¨**: ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç APIå¯†é’¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
2. **é€Ÿç‡é™åˆ¶**: æ³¨æ„å„Providerçš„é€Ÿç‡é™åˆ¶ï¼Œå®ç°é€‚å½“çš„é‡è¯•é€»è¾‘
3. **æˆæœ¬æ§åˆ¶**: ç›‘æ§tokenä½¿ç”¨é‡ï¼Œç‰¹åˆ«æ˜¯æ˜‚è´µçš„æ¨¡å‹
4. **é”™è¯¯å¤„ç†**: å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
5. **å¼‚æ­¥ä½¿ç”¨**: æ‰€æœ‰APIéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œç¡®ä¿æ­£ç¡®ä½¿ç”¨`await`

## ç¤ºä¾‹ä»£ç 

æŸ¥çœ‹ `app/llm/example.py` è·å–æ›´å¤šä½¿ç”¨ç¤ºä¾‹ã€‚
